{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from scipy.stats import poisson, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change define the leaky_relu with default parameter `alpha=0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = partial(tf.nn.leaky_relu,alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "lambda_gradient = 0.025 # Gradient penalty\n",
    "lambda_entropy = 3. # Entropy penalty\n",
    "obs_alpha = 7. # Mean of first component of the simulator\n",
    "prop_initial_mu = 0. # Initial mu of the proposal distribution \n",
    "prop_initial_log_sigma = 0. # Inintial log(sigma) of the proposal distribution\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "critic_steps = 100\n",
    "count_steps = 500+1\n",
    "\n",
    "#For reproducibility \n",
    "tf.set_random_seed(2210)\n",
    "np.random.seed(2210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define simulator and generate observed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define semipositive matrix as in paper\n",
    "R = np.array([\n",
    "    [1.31229955,  0.10499961,  0.48310515, -0.3249938,  -0.26387927],\n",
    "    [0.10499961,  1.15833058, -0.55865473,  0.25275522, -0.39790775],\n",
    "    [0.48310515, -0.55865473,  2.25874579, -0.52087938, -0.39271231],\n",
    "    [0.3249938,   0.25275522, -0.52087938,  1.4034925,  -0.63521059],\n",
    "    [-0.26387927, -0.39790775, -0.39271231, -0.63521059, 1.]])\n",
    "\n",
    "# Define simulator\n",
    "def simulator(alpha,beta):\n",
    "    try:\n",
    "        n = len(alpha)\n",
    "    except TypeError:\n",
    "        n = 1\n",
    "    z0 = np.random.normal(loc=alpha.flatten())\n",
    "    z1 = np.random.normal(loc=beta.flatten(),scale=3)\n",
    "    z21 = np.random.normal(loc=-2,size=n)\n",
    "    z22 = np.random.normal(loc=2,scale=0.5,size=n)\n",
    "    r = np.random.choice([True, False], n)\n",
    "    z2 = np.where(r, z21, z22)\n",
    "    z3 = np.random.exponential(scale=3.,size=n)\n",
    "    z4 = np.random.exponential(scale=0.5,size=n)\n",
    "    return np.row_stack([z0,z1,z2,z3,z4]).T.dot(R)\n",
    "\n",
    "#Generate real sample pool\n",
    "N_obs = 200000\n",
    "real_samples = simulator(np.array([N_obs*[1.]]).T,np.array([N_obs*[-1.]]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define AVO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_shape = (batch_size,1)\n",
    "batch_shape = (batch_size, 5)\n",
    "critic_shape = (batch_size, 1)\n",
    "\n",
    "# Placeholder for real distribution\n",
    "X_real = tf.placeholder(tf.float32,shape=batch_shape)\n",
    "\n",
    "\n",
    "# Define proposal parameters Psi for alpha (a) and beta (b)\n",
    "with tf.variable_scope('proposal'):\n",
    "    mu_a = tf.get_variable('mu_a', shape=(), initializer=tf.constant_initializer(prop_initial_mu))\n",
    "    # We use logarithm of sigma since sigma >= 0 in order to stretch it over the real line\n",
    "    lg_sigma_a = tf.get_variable('lg_sigma_a', shape=(), initializer=tf.constant_initializer(prop_initial_log_sigma))\n",
    "    mu_b = tf.get_variable('mu_b', shape=(), initializer=tf.constant_initializer(prop_initial_mu))\n",
    "    lg_sigma_b = tf.get_variable('lg_sigma_b', shape=(), initializer=tf.constant_initializer(prop_initial_log_sigma))\n",
    "\n",
    "    \n",
    "# Define proposal distribution q\n",
    "proposal_distribution_a = tf.contrib.distributions.Normal(loc=mu_a, scale=tf.exp(lg_sigma_a))\n",
    "proposal_distribution_b = tf.contrib.distributions.Normal(loc=mu_b, scale=tf.exp(lg_sigma_b))\n",
    "sample_proposal_op_a = tf.stop_gradient(proposal_distribution_a.sample(sample_shape=proposal_shape))\n",
    "sample_proposal_op_b = tf.stop_gradient(proposal_distribution_b.sample(sample_shape=proposal_shape))\n",
    "\n",
    "# Compute the log probability for the parameters \n",
    "log_prob_prop = proposal_distribution_a.log_prob(sample_proposal_op_a)+\\\n",
    "                 proposal_distribution_b.log_prob(sample_proposal_op_b)\n",
    "#Analytic differential entropy for the proposal distribution\n",
    "entropy_proposal = lg_sigma_a+lg_sigma_b\n",
    "\n",
    "# Placeholder for simulated distribution\n",
    "X_sim = tf.placeholder(tf.float32,shape=batch_shape)\n",
    "\n",
    "# Define the critic\n",
    "def critic(x):\n",
    "    layer = slim.stack(x, slim.fully_connected, [10, 10], scope='shared', activation_fn=leaky_relu)\n",
    "    logits = slim.fully_connected(layer, 1, activation_fn=tf.identity, scope='logits')\n",
    "    return logits\n",
    "\n",
    "# Define interpolated data points for the Gradient Penalty Term\n",
    "eps = tf.random_uniform(critic_shape, minval=0., maxval=1.)\n",
    "X_interp = eps*X_real + (1.-eps)*X_sim\n",
    "\n",
    "# Compute critic for different inputs sharing the same variables of the NN:\n",
    "with tf.variable_scope('critic'):\n",
    "        critic_real = critic(X_real)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        critic_sim = critic(X_sim)\n",
    "        critic_interp = critic(X_interp)\n",
    "\n",
    "# Gradient penalty \n",
    "grad = tf.gradients(critic_interp, [X_interp])[0]\n",
    "gradient_penalty = lambda_gradient * tf.square(tf.norm(grad, 2,axis=1) - 1)\n",
    "\n",
    "# Define losses\n",
    "loss_critic = tf.reduce_mean(critic_sim - critic_real + gradient_penalty)\n",
    "wgan_loss = -tf.reduce_mean(critic_sim - critic_real) # Distance between distributions\n",
    "loss_prop = tf.reduce_mean(-tf.multiply(critic_sim, log_prob_prop) + lambda_entropy*entropy_proposal)\n",
    "\n",
    "# Define trainable variables\n",
    "prop_vars = [var for var in tf.trainable_variables() if var.name.startswith('proposal')]\n",
    "critic_vars = [var for var in tf.trainable_variables() if var.name.startswith('critic')]\n",
    "\n",
    "# Define optimizers\n",
    "critic_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='critic')\n",
    "prop_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='proposal')\n",
    "\n",
    "# Define training operations\n",
    "train_critic = critic_optimizer.minimize(loss_critic, var_list=critic_vars)\n",
    "train_proposal = prop_optimizer.minimize(loss_prop, var_list=prop_vars)\n",
    "# Define variables to reset for critic optimizer\n",
    "velocity_vars = [critic_optimizer.get_slot(var, 'v') for var in critic_vars]\n",
    "momentum_vars = [critic_optimizer.get_slot(var, 'm') for var in critic_vars]\n",
    "reset_vel_mom = tf.variables_initializer(velocity_vars + momentum_vars)\n",
    "\n",
    "#Reset beta power variables:\n",
    "reset_b1 = tf.assign(critic_optimizer._beta1_power,beta1)\n",
    "reset_b2 = tf.assign(critic_optimizer._beta2_power,beta2)\n",
    "reset_critic = [reset_b1,reset_b2,reset_vel_mom]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run AVO model to find the parameter distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: mu_a = 0.0, sigma_a = 1.0, mu_b = 0.0, sigma_b = 1.0\n",
      "Step 1: mu_a = 0.010000000707805157, sigma_a = 1.0100501775741577, mu_b = -0.009999999776482582, sigma_b = 1.0100501775741577, loss = 51.524932861328125\n",
      "Step 11: mu_a = 0.08904185146093369, sigma_a = 1.0157819986343384, mu_b = -0.08384522795677185, sigma_b = 1.0554512739181519, loss = 90.0028076171875\n",
      "Step 21: mu_a = 0.16901929676532745, sigma_a = 1.0073647499084473, mu_b = -0.16622188687324524, sigma_b = 1.0388904809951782, loss = 89.91385650634766\n",
      "Step 31: mu_a = 0.25653719902038574, sigma_a = 0.9851034879684448, mu_b = -0.23532892763614655, sigma_b = 1.0244684219360352, loss = 58.791603088378906\n",
      "Step 41: mu_a = 0.3377964496612549, sigma_a = 0.9695329666137695, mu_b = -0.27610480785369873, sigma_b = 1.023491621017456, loss = 67.99066162109375\n",
      "Step 51: mu_a = 0.4062129557132721, sigma_a = 0.9522089958190918, mu_b = -0.32066935300827026, sigma_b = 1.017338514328003, loss = 26.9014835357666\n",
      "Step 61: mu_a = 0.4565171003341675, sigma_a = 0.9248540997505188, mu_b = -0.3773946166038513, sigma_b = 0.9988343715667725, loss = -16.689014434814453\n",
      "Step 71: mu_a = 0.501411497592926, sigma_a = 0.9156656265258789, mu_b = -0.44369077682495117, sigma_b = 0.971753716468811, loss = 6.383181571960449\n",
      "Step 81: mu_a = 0.5412432551383972, sigma_a = 0.9099634885787964, mu_b = -0.503720223903656, sigma_b = 0.9533349275588989, loss = 28.990360260009766\n",
      "Step 91: mu_a = 0.5823314189910889, sigma_a = 0.8898560404777527, mu_b = -0.5602253079414368, sigma_b = 0.9451804161071777, loss = -6.535550117492676\n",
      "Step 101: mu_a = 0.6255192756652832, sigma_a = 0.8753325939178467, mu_b = -0.5910942554473877, sigma_b = 0.9233084321022034, loss = 8.490865707397461\n",
      "Step 111: mu_a = 0.6499793529510498, sigma_a = 0.8761062622070312, mu_b = -0.6333508491516113, sigma_b = 0.9162905812263489, loss = 13.124044418334961\n",
      "Step 121: mu_a = 0.6783983707427979, sigma_a = 0.8844687342643738, mu_b = -0.6856710910797119, sigma_b = 0.9137173891067505, loss = -0.8727588653564453\n",
      "Step 131: mu_a = 0.6983749270439148, sigma_a = 0.869120180606842, mu_b = -0.7241270542144775, sigma_b = 0.9196129441261292, loss = 2.7198572158813477\n",
      "Step 141: mu_a = 0.7213981747627258, sigma_a = 0.8456827402114868, mu_b = -0.7445238828659058, sigma_b = 0.9198665022850037, loss = 11.34230899810791\n",
      "Step 151: mu_a = 0.7727118134498596, sigma_a = 0.8173630833625793, mu_b = -0.7589538097381592, sigma_b = 0.9172164797782898, loss = 7.725764274597168\n",
      "Step 161: mu_a = 0.8192515969276428, sigma_a = 0.8085899949073792, mu_b = -0.7947103381156921, sigma_b = 0.9126565456390381, loss = -9.223270416259766\n",
      "Step 171: mu_a = 0.8544607162475586, sigma_a = 0.8010878562927246, mu_b = -0.8331365585327148, sigma_b = 0.917762279510498, loss = 2.2513504028320312\n",
      "Step 181: mu_a = 0.8767229914665222, sigma_a = 0.7767267823219299, mu_b = -0.8570712208747864, sigma_b = 0.9300563335418701, loss = 1.7823634147644043\n",
      "Step 191: mu_a = 0.8886626958847046, sigma_a = 0.7652770280838013, mu_b = -0.8818374872207642, sigma_b = 0.9327767491340637, loss = 2.989821195602417\n",
      "Step 201: mu_a = 0.8931118249893188, sigma_a = 0.7564895153045654, mu_b = -0.9037361741065979, sigma_b = 0.9303677082061768, loss = -3.4879982471466064\n",
      "Step 211: mu_a = 0.8964387774467468, sigma_a = 0.7443335056304932, mu_b = -0.9168264865875244, sigma_b = 0.9176661372184753, loss = 1.3847092390060425\n",
      "Step 221: mu_a = 0.9026447534561157, sigma_a = 0.7291005253791809, mu_b = -0.9251503348350525, sigma_b = 0.908780574798584, loss = -1.470562219619751\n",
      "Step 231: mu_a = 0.9061979651451111, sigma_a = 0.7190597057342529, mu_b = -0.9363272786140442, sigma_b = 0.8991930484771729, loss = 1.6879159212112427\n",
      "Step 241: mu_a = 0.9167600274085999, sigma_a = 0.7040429711341858, mu_b = -0.9477732181549072, sigma_b = 0.8973569869995117, loss = 6.182847023010254\n",
      "Step 251: mu_a = 0.9259887933731079, sigma_a = 0.6854879260063171, mu_b = -0.9590637683868408, sigma_b = 0.8957401514053345, loss = 1.2211904525756836\n",
      "Step 261: mu_a = 0.9288052320480347, sigma_a = 0.6708141565322876, mu_b = -0.9645785093307495, sigma_b = 0.8855928182601929, loss = -0.5513564348220825\n",
      "Step 271: mu_a = 0.9373860359191895, sigma_a = 0.6617798805236816, mu_b = -0.9735146760940552, sigma_b = 0.8775615692138672, loss = 9.075460433959961\n",
      "Step 281: mu_a = 0.9473502039909363, sigma_a = 0.6545819640159607, mu_b = -0.9850049018859863, sigma_b = 0.8680667281150818, loss = 2.990272045135498\n",
      "Step 291: mu_a = 0.9556341767311096, sigma_a = 0.6466572880744934, mu_b = -0.9944027662277222, sigma_b = 0.8515873551368713, loss = -0.11855125427246094\n",
      "Step 301: mu_a = 0.9676724672317505, sigma_a = 0.6388037800788879, mu_b = -1.0004611015319824, sigma_b = 0.840984582901001, loss = 3.5147674083709717\n",
      "Step 311: mu_a = 0.9732652902603149, sigma_a = 0.6257489919662476, mu_b = -1.0065429210662842, sigma_b = 0.829874575138092, loss = 1.5517678260803223\n",
      "Step 321: mu_a = 0.9747983813285828, sigma_a = 0.6160252094268799, mu_b = -1.0134259462356567, sigma_b = 0.8186503648757935, loss = -3.274177312850952\n",
      "Step 331: mu_a = 0.9779052734375, sigma_a = 0.60950767993927, mu_b = -1.0186282396316528, sigma_b = 0.8119382858276367, loss = -1.4875469207763672\n",
      "Step 341: mu_a = 0.9831345677375793, sigma_a = 0.5999391078948975, mu_b = -1.0249831676483154, sigma_b = 0.8027833104133606, loss = 1.9742075204849243\n",
      "Step 351: mu_a = 0.9942723512649536, sigma_a = 0.5900757908821106, mu_b = -1.0370675325393677, sigma_b = 0.7932982444763184, loss = 2.5865631103515625\n",
      "Step 361: mu_a = 1.0040193796157837, sigma_a = 0.5801786184310913, mu_b = -1.0442265272140503, sigma_b = 0.7839683294296265, loss = 0.22958225011825562\n",
      "Step 371: mu_a = 1.0084877014160156, sigma_a = 0.568869411945343, mu_b = -1.040964961051941, sigma_b = 0.7762183547019958, loss = -0.6644715666770935\n",
      "Step 381: mu_a = 1.0119436979293823, sigma_a = 0.5606141686439514, mu_b = -1.0319764614105225, sigma_b = 0.7630069255828857, loss = 1.0980104207992554\n",
      "Step 391: mu_a = 1.0083562135696411, sigma_a = 0.5527896285057068, mu_b = -1.021498680114746, sigma_b = 0.7524439692497253, loss = 2.8033270835876465\n",
      "Step 401: mu_a = 1.0045198202133179, sigma_a = 0.544608473777771, mu_b = -1.0138342380523682, sigma_b = 0.7434641718864441, loss = 0.4678342938423157\n",
      "Step 411: mu_a = 0.999903678894043, sigma_a = 0.5377404093742371, mu_b = -1.0088579654693604, sigma_b = 0.7341877222061157, loss = 3.7386631965637207\n",
      "Step 421: mu_a = 0.9955232739448547, sigma_a = 0.5295619964599609, mu_b = -1.0037646293640137, sigma_b = 0.7221711874008179, loss = -0.5641642212867737\n",
      "Step 431: mu_a = 1.00135338306427, sigma_a = 0.5204890370368958, mu_b = -1.0026317834854126, sigma_b = 0.7122751474380493, loss = 0.7965173125267029\n",
      "Step 441: mu_a = 1.0007814168930054, sigma_a = 0.510290265083313, mu_b = -1.0032238960266113, sigma_b = 0.7043893933296204, loss = 0.9286686182022095\n",
      "Step 451: mu_a = 1.003958821296692, sigma_a = 0.5003984570503235, mu_b = -1.002075433731079, sigma_b = 0.6952435970306396, loss = -0.7865384221076965\n",
      "Step 461: mu_a = 1.007441759109497, sigma_a = 0.49416157603263855, mu_b = -1.006103277206421, sigma_b = 0.6877737045288086, loss = 0.6942872405052185\n",
      "Step 471: mu_a = 1.0064646005630493, sigma_a = 0.4870266616344452, mu_b = -1.0075290203094482, sigma_b = 0.6804404854774475, loss = 2.0664620399475098\n",
      "Step 481: mu_a = 1.0097328424453735, sigma_a = 0.4781236946582794, mu_b = -1.0058643817901611, sigma_b = 0.6730570793151855, loss = -1.2350647449493408\n",
      "Step 491: mu_a = 1.0111808776855469, sigma_a = 0.47040003538131714, mu_b = -1.0011392831802368, sigma_b = 0.6619046926498413, loss = 1.3002614974975586\n",
      "Step 501: mu_a = 1.0121939182281494, sigma_a = 0.4626028835773468, mu_b = -1.0052160024642944, sigma_b = 0.6518608927726746, loss = -0.16176149249076843\n"
     ]
    }
   ],
   "source": [
    "# Save losses\n",
    "wgan_losses = np.zeros(count_steps)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Output the current values\n",
    "    mu_a_val, lg_sigma_a_val, mu_b_val, lg_sigma_b_val = sess.run([mu_a, lg_sigma_a, mu_b, lg_sigma_b])\n",
    "    print('Step 0: mu_a = {}, sigma_a = {}, mu_b = {}, sigma_b = {}'.format(mu_a_val, np.exp(lg_sigma_a_val),\n",
    "                                                                            mu_b_val, np.exp(lg_sigma_b_val)))\n",
    "\n",
    "    for epoch in range(count_steps):\n",
    "        # Reset critic optimizer parameters at each epoch\n",
    "        sess.run(reset_critic)\n",
    "\n",
    "        # Optimize for the EM metric\n",
    "        for idx in range(0, critic_steps):\n",
    "            x_real = real_samples[np.random.choice(N_obs,size=batch_size)]\n",
    "            alphas,betas = sess.run([sample_proposal_op_a,sample_proposal_op_b])\n",
    "            x_sim = simulator(alphas,betas)\n",
    "            _, this_loss = sess.run([train_critic, wgan_loss],\n",
    "                                    feed_dict={X_real:x_real,\n",
    "                                               X_sim:x_sim})\n",
    "        # Save loss after optimizing\n",
    "        wgan_losses[epoch] = this_loss\n",
    "\n",
    "        # Update the proposal\n",
    "        alphas,betas = sess.run([sample_proposal_op_a,sample_proposal_op_b])\n",
    "        x_sim = simulator(alphas,betas)\n",
    "        _ = sess.run(train_proposal,\n",
    "                     feed_dict={X_sim:x_sim,\n",
    "                                sample_proposal_op_a:alphas,\n",
    "                                sample_proposal_op_b:betas})\n",
    "        # Output the current values\n",
    "        if epoch % 10 == 0:\n",
    "            mu_a_val, lg_sigma_a_val, mu_b_val, lg_sigma_b_val = sess.run([mu_a, lg_sigma_a, mu_b, lg_sigma_b])\n",
    "            print('Step {}: mu_a = {}, sigma_a = {}, mu_b = {}, sigma_b = {}, loss = {}'.format(epoch+1,mu_a_val, np.exp(lg_sigma_a_val),\n",
    "                                                                                     mu_b_val, np.exp(lg_sigma_b_val), this_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
